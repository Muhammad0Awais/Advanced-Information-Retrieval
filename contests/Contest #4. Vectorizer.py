# -*- coding: utf-8 -*-
"""2021S - Vector Space Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FHZGNLrE4kClsG-JHC1hbvneXELE5iou

# Words, concepts and search

You are given a collection of texts, which form a dataset. Each string means a separate document. For each test we will answer 2 following questions:
- Which of the documents is the closest to a given query?
- How many "concepts" is enough to represent these tests?

Thus your result (answer) will consist of 2 integers, separated by a space: `doc_id concept_count`.

## Let's consider the test example:

`input.txt`

```
c d b.
d e a.
a b c.
a b c d.
d c a b.
a c.      # <--- the last one is the query
```

## Let's do vectorization
Reuse this in your solutions
"""

from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

with open('input.txt', 'r') as fin:
    inp = fin.read()

inp = inp.strip().split("\n")

dataset, query = inp[:-1], inp[-1]

vect = TfidfVectorizer(
            analyzer='word',
            stop_words=None,
            token_pattern=r"(?u)\b\w+\b"    # (?u)\b\w\w+\b -- default pattern: (?u) -- unicode modifier, \b -- word border, \w\w+ = 2+ letters
)
DTM = vect.fit_transform(dataset).todense()

"""### So, we are ready to answer question 1. 
Which of the documents is the closest to a given query?
"""

def getSimilarity(query_vector, DTM):
  sim = []
  for k in DTM:
    sim.append((np.dot( query_vector,  k.T) / (np.linalg.norm(query_vector) * np.linalg.norm(k))).A1.tolist())
  return sim

def findBest(sim):
  return np.argmax(sim)

query_vector = vect.transform([query]).todense()
cosines_raw = np.array(getSimilarity(query_vector, DTM))

"""### Time for question 2.
How many concepts are enough to express our dataset?

In other words, how many orthogonal components do we need to pass `allclose` test?

**NB: Can you just take the data and run PCA? Will it change the cosine metric?**

Implement reduced SVD to pass the test.
"""

from numpy.linalg import svd

U, sigma, Vh = np.linalg.svd(DTM, full_matrices=True)
Sigma = np.diag(sigma)

for i in range(1, min(DTM.shape[1], DTM.shape[0]) + 1):
    doc_embeddings = np.dot(U[:, :i] , Sigma[:i,:i]) # ...
    projection = Vh[:i,:]     # ...
    DTM_approx = np.dot(doc_embeddings, projection)     # ...
    # print(f"If we take {i} components, allclose =", np.allclose(DTM, DTM_approx, atol=0.01))
    if np.allclose(DTM, DTM_approx, atol=0.01):
      k = i
      break

# k = 5

# doc_embeddings =  np.dot(U[:, :k] , Sigma[:k,:k]) # ...  
# projection = Vh[:k,:]     # ...
# doc_emb = np.dot(doc_embeddings, projection)
# query_embedding = projection @ query_vector.T

# cosines = doc_embeddings @ query_embedding

# print(f"Cosine similarities of query and dataset (after reduction to {k} dimensions):\n", cosines)
# assert np.allclose(cosines, cosines_raw), "Cosine similariries are not close"
# print("Best match index: ", findBest(cosines))

bestMatch = findBest(cosines_raw)
stra = str(bestMatch) + " " + str(k)

with open('output.txt', 'w') as fout:
    fout.write(stra)

"""Exactly the same! We verified the job!

### And the answer is...

Thus, looking at the result of previous block, we can state:
    
`output.txt`
```
2 4
```
"""
